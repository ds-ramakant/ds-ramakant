---
title: "Day 6 of 50daysofkaggle"
author: ''
date: "2022-10-12"
slug: "day-6-of-50daysofkaggle"
categories: kaggle
tags:
- 50daysofkaggle
- kaggle
- machinelearning
- python
subtitle: Classification using KNN
summary: ''
authors: []
lastmod: "2022-10-12T15:38:17+05:30"
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

# Day 6: The Titanic Dataset

Progress till date:

-   Download titanic dataset and assign to `train` & `test`
-   Rearranging the data
-   EDA

To do today:

-   write function to find share of survivors by each variable
-   attempt to create model

```{python}
import requests
import numpy as np
import pandas as pd
import kaggle 
import zipfile 

kaggle.api.authenticate()

kaggle.api.competition_download_files("titanic", path = ".")

zf = zipfile.ZipFile("titanic.zip")
train = pd.read_csv(zf.open("train.csv"))
test = pd.read_csv(zf.open("test.csv"))

#Selecting only the numerical columns
num_col = train.select_dtypes(include=np.number).columns.tolist()

#deslecting passenger ID and 'Survived' 
del num_col[0:2] #.remove() can remove only 1 item. so for more than 1, use for loop 
select_col = num_col

#remaining columns
str_col= ["Sex", "Embarked", "Survived"]
str_col

#Adding more elements into a list using `extend` and not `append`
select_col.extend(str_col)
select_col

train_eda= train[train.columns.intersection(select_col)]
train_eda

train_eda.isna().sum().sort_values()
```
Almost 177 entries in the `Age` column have no value. Replacing these with the median age instead of removing them. 

```{python}

train_eda["Age"].median() #28

train_eda["Age"].fillna(value = train_eda["Age"].median(), inplace = True)

train_eda.isna().sum().sort_values()

```



Today I want to calculate the survival rate of each of these attributes (`Pclass, Sex, Embarked`).

```{python}

df_copy2 = pd.DataFrame(columns = {"category", "col", "survive_rate"})

for t in ["Pclass", "Sex", "Embarked"]:
  df_copy = train_eda.groupby([t])["Survived"].mean().reset_index()
  df_copy["category"] = t
  #trying to create a `tidy` version of the data 
  df_copy.rename(columns = {t: "col", "Survived": "survive_rate"}, errors = "raise", inplace = True)
  df_copy = df_copy[["category", "col", "survive_rate"]]
  df_copy2= pd.concat([df_copy2, df_copy], ignore_index = True)


#final table in a tidy format that can be used to create graphs. but that i'm keeping for later
df_copy2[["category", "col", "survive_rate"]]
```

With this, its pretty clear that among the `sex` category, males had the least likelihood of surviving with 19%. The richer `class 1` managed a 63% chance of survival while only 24% of the lower `class 3` survived. Finally those that `embarked` from Cherbourg had a higher survival rate 55% compared to Southampton at 34%.


## Model building

Seperating the X & y

```{python}

train_eda.isna().sum().sort_values()


train_eda = train_eda.dropna(axis = 0) #removing all rows with NA

X = train_eda[["Age", "SibSp", "Parch", "Fare"]]
X


X = pd.concat([X,pd.get_dummies(data = train_eda[["Sex", "Embarked", "Pclass"]], columns = ["Sex", "Embarked", "Pclass"])], axis = 1)

X.head()

y = train_eda["Survived"].values
y[0:5]

len(y) #889 after filling up the NA. previously 712
X.shape #(889, 12)
```

### Normalising the data

```{python}
from sklearn import preprocessing

X= preprocessing.StandardScaler().fit(X).transform(X)
X[0:5]
```

### Splitting into Test & Train data

```{python}

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)
print ('Train set\t :', X_train.shape,  y_train.shape,
'\nTest set\t :', X_test.shape,  y_test.shape)


```

### K Nearest Neighbours

```{python}

from sklearn.neighbors import KNeighborsClassifier

k = 4

neighbours = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
neighbours
```

#### Predicting the output `yhat` and checking accuracy

```{python}

yhat1 = neighbours.predict(X_test)
yhat1[0:5]
```

```{python}
from sklearn import metrics

print("Train set Accuracy \t:", metrics.accuracy_score(y_train, neighbours.predict(X_train)), "\nTest set Accuracy \t:", metrics.accuracy_score(y_test, yhat1))

```
*(without replacing `na` values, the previous test accuracy was 78%)*
#### Checking for other K

```{python}
from sklearn import metrics

Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))

for n in range(1,Ks):
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc
```

By now you'd have figured that all this is pretty much copy-pasta from someplace else. Glad that IBM coursera assignments came in handy!

```{python}
import matplotlib.pyplot as plt

plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.fill_between(range(1,Ks),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color="green")
plt.legend(('Accuracy ', '+/- 1xstd','+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Neighbors (K)')
plt.tight_layout()
plt.show()
```

Looks like accuracy of KNN is best at 7 neighbours.
*previously without replacing NA the accuracy was highest at k = 5*

#### Redo with `K = 5`

```{python}
k = 7

neighbours_7 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
yhat = neighbours_7.predict(X_test)

print("Train set Accuracy \t:", 
      metrics.accuracy_score(y_train, neighbours_7.predict(X_train)),
      "\nTest set Accuracy \t:", 
      metrics.accuracy_score(y_test, yhat))
```
We find that Test accuracy is around **80% for KNN** 
*pretty much the same as previous attempt before replacing NA*